import os
import torch
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    TextIteratorStreamer,
)
from threading import Thread

# =====================
# Config
# =====================
MODEL_ID = os.getenv("MODEL_ID", "Milkfish033/deepseek-r1-1.5b-merged")

SYSTEM_PROMPT = (
    "ä½ æ˜¯ Belloï¼Œä¸€ä¸ªå‹å¥½çš„æ™ºèƒ½åŠ©æ‰‹ã€‚"
    "ä½ æ“…é•¿å›ç­”ä¸çª—æˆ·äº§å“ã€ä½¿ç”¨åœºæ™¯å’Œé€‰å‹ç›¸å…³çš„é—®é¢˜ã€‚"
    "è¯·ç”¨æ¸…æ™°ã€ç®€æ´çš„ä¸­æ–‡å›ç­”ç”¨æˆ·é—®é¢˜ã€‚"
)

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
DTYPE = torch.float16 if DEVICE == "cuda" else torch.float32

# =====================
# Load model
# =====================
print("ğŸ”„ æ­£åœ¨åŠ è½½æ¨¡å‹...")
tokenizer = AutoTokenizer.from_pretrained(
    MODEL_ID,
    trust_remote_code=True,
)

model = AutoModelForCausalLM.from_pretrained(
    MODEL_ID,
    torch_dtype=DTYPE,
    device_map="auto" if DEVICE == "cuda" else None,
    trust_remote_code=True,
)
model.eval()
print("âœ… æ¨¡å‹åŠ è½½å®Œæˆ")

# =====================
# Chat loop
# =====================
def build_prompt(messages):
    """
    æ ¹æ®å¯¹è¯å†å²æ„é€  prompt
    å…¼å®¹ deepseek / qwen / llama é£æ ¼
    """
    prompt = f"<|system|>\n{SYSTEM_PROMPT}\n"
    for role, content in messages:
        if role == "user":
            prompt += f"<|user|>\n{content}\n"
        else:
            prompt += f"<|assistant|>\n{content}\n"
    prompt += "<|assistant|>\n"
    return prompt


def chat():
    messages = []

    print("\nğŸ’¬ Bello å·²ä¸Šçº¿ï¼ˆè¾“å…¥ exit é€€å‡ºï¼‰\n")

    while True:
        user_input = input("ä½ ï¼š").strip()
        if user_input.lower() in {"exit", "quit"}:
            print("ğŸ‘‹ å†è§ï¼")
            break

        messages.append(("user", user_input))

        prompt = build_prompt(messages)

        inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

        streamer = TextIteratorStreamer(
            tokenizer,
            skip_prompt=True,
            skip_special_tokens=True,
        )

        generation_kwargs = dict(
            **inputs,
            streamer=streamer,
            max_new_tokens=512,
            temperature=0.7,
            top_p=0.9,
            do_sample=True,
        )

        thread = Thread(target=model.generate, kwargs=generation_kwargs)
        thread.start()

        print("Belloï¼š", end="", flush=True)
        assistant_output = ""
        for token in streamer:
            print(token, end="", flush=True)
            assistant_output += token
        print()

        messages.append(("assistant", assistant_output))


if __name__ == "__main__":
    chat()
